[
  {
    "objectID": "wine_quality_prediction.html",
    "href": "wine_quality_prediction.html",
    "title": "Wine Quality Prediction Using Stacking Model And Hyperparameter Tuning",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n# Loading the necessary packages\nimport random\nfrom seaborn.palettes import color_palette\nrandom.seed(1296)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
  },
  {
    "objectID": "wine_quality_prediction.html#import-packages",
    "href": "wine_quality_prediction.html#import-packages",
    "title": "Wine Quality Prediction Using Stacking Model And Hyperparameter Tuning",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n# Loading the necessary packages\nimport random\nfrom seaborn.palettes import color_palette\nrandom.seed(1296)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
  },
  {
    "objectID": "wine_quality_prediction.html#data",
    "href": "wine_quality_prediction.html#data",
    "title": "Wine Quality Prediction Using Stacking Model And Hyperparameter Tuning",
    "section": "Data",
    "text": "Data\n\n# Load the dataset\nwq_data = read_csv(\"dataset/winequality-red.csv\",header=0)\nstyle = [\n    {'selector': 'th', 'props': [\n        ('background-color', '#373c41ff'),\n        ('color', 'white'),\n        ('font-weight', 'bold')\n    ]}\n]\ndisplay(wq_data.head(5).style.set_table_styles(style).format(\"{:.2f}\"))\ndisplay(wq_data.describe().style.set_table_styles(style).format(\"{:.2f}\"))\n\n\n\n\n\n\n \nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.40\n0.70\n0.00\n1.90\n0.08\n11.00\n34.00\n1.00\n3.51\n0.56\n9.40\n5.00\n\n\n1\n7.80\n0.88\n0.00\n2.60\n0.10\n25.00\n67.00\n1.00\n3.20\n0.68\n9.80\n5.00\n\n\n2\n7.80\n0.76\n0.04\n2.30\n0.09\n15.00\n54.00\n1.00\n3.26\n0.65\n9.80\n5.00\n\n\n3\n11.20\n0.28\n0.56\n1.90\n0.07\n17.00\n60.00\n1.00\n3.16\n0.58\n9.80\n6.00\n\n\n4\n7.40\n0.70\n0.00\n1.90\n0.08\n11.00\n34.00\n1.00\n3.51\n0.56\n9.40\n5.00\n\n\n\n\n\n\n\n\n\n\n \nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\ncount\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n1599.00\n\n\nmean\n8.32\n0.53\n0.27\n2.54\n0.09\n15.87\n46.47\n1.00\n3.31\n0.66\n10.42\n5.64\n\n\nstd\n1.74\n0.18\n0.19\n1.41\n0.05\n10.46\n32.90\n0.00\n0.15\n0.17\n1.07\n0.81\n\n\nmin\n4.60\n0.12\n0.00\n0.90\n0.01\n1.00\n6.00\n0.99\n2.74\n0.33\n8.40\n3.00\n\n\n25%\n7.10\n0.39\n0.09\n1.90\n0.07\n7.00\n22.00\n1.00\n3.21\n0.55\n9.50\n5.00\n\n\n50%\n7.90\n0.52\n0.26\n2.20\n0.08\n14.00\n38.00\n1.00\n3.31\n0.62\n10.20\n6.00\n\n\n75%\n9.20\n0.64\n0.42\n2.60\n0.09\n21.00\n62.00\n1.00\n3.40\n0.73\n11.10\n6.00\n\n\nmax\n15.90\n1.58\n1.00\n15.50\n0.61\n72.00\n289.00\n1.00\n4.01\n2.00\n14.90\n8.00\n\n\n\n\n\nThere are 11 alcohol features in the dataset with 1 variable “quality” representing the evaluated alcohol quality that we are going to predict. The quality scores range from 3 to 8 in the dataset."
  },
  {
    "objectID": "project_step2_grp8.html",
    "href": "project_step2_grp8.html",
    "title": "Red Wine Quality Prediction Model",
    "section": "",
    "text": "Team 8 - Jiazu Zhang - jz944\n\n\nTeam 8 - Mengjia Wei - mw1296\n\n\nTeam 8 - Ahmed Khair - afk46\n\n\n3. Analysis of the dataset and Trained Model\n\n3.1 Load dataset and packages\n\n# Loading the necessary packages\n\nimport random\nfrom seaborn.palettes import color_palette\nrandom.seed(9001)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline\n\n# Load the dataset\ngr8_wq= pd.read_csv(\"dataset/winequality-red.csv\",header=0)\n\n\n\n3.2 Show first rows of dataset and describe the dataframe\n\n# Show first 10 rows of the dataframe\ngr8_wq.head(11)\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n5\n7.4\n0.66\n0.00\n1.8\n0.075\n13.0\n40.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n6\n7.9\n0.60\n0.06\n1.6\n0.069\n15.0\n59.0\n0.9964\n3.30\n0.46\n9.4\n5\n\n\n7\n7.3\n0.65\n0.00\n1.2\n0.065\n15.0\n21.0\n0.9946\n3.39\n0.47\n10.0\n7\n\n\n8\n7.8\n0.58\n0.02\n2.0\n0.073\n9.0\n18.0\n0.9968\n3.36\n0.57\n9.5\n7\n\n\n9\n7.5\n0.50\n0.36\n6.1\n0.071\n17.0\n102.0\n0.9978\n3.35\n0.80\n10.5\n5\n\n\n10\n6.7\n0.58\n0.08\n1.8\n0.097\n15.0\n65.0\n0.9959\n3.28\n0.54\n9.2\n5\n\n\n\n\n\n\n\n\n# Describe dataset\ngr8_wq.describe()\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\ncount\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n1599.000000\n\n\nmean\n8.319637\n0.527821\n0.270976\n2.538806\n0.087467\n15.874922\n46.467792\n0.996747\n3.311113\n0.658149\n10.422983\n5.636023\n\n\nstd\n1.741096\n0.179060\n0.194801\n1.409928\n0.047065\n10.460157\n32.895324\n0.001887\n0.154386\n0.169507\n1.065668\n0.807569\n\n\nmin\n4.600000\n0.120000\n0.000000\n0.900000\n0.012000\n1.000000\n6.000000\n0.990070\n2.740000\n0.330000\n8.400000\n3.000000\n\n\n25%\n7.100000\n0.390000\n0.090000\n1.900000\n0.070000\n7.000000\n22.000000\n0.995600\n3.210000\n0.550000\n9.500000\n5.000000\n\n\n50%\n7.900000\n0.520000\n0.260000\n2.200000\n0.079000\n14.000000\n38.000000\n0.996750\n3.310000\n0.620000\n10.200000\n6.000000\n\n\n75%\n9.200000\n0.640000\n0.420000\n2.600000\n0.090000\n21.000000\n62.000000\n0.997835\n3.400000\n0.730000\n11.100000\n6.000000\n\n\nmax\n15.900000\n1.580000\n1.000000\n15.500000\n0.611000\n72.000000\n289.000000\n1.003690\n4.010000\n2.000000\n14.900000\n8.000000\n\n\n\n\n\n\n\n\n\n3.3 Create pairplot for whole dataset to observe the distributions and correlations\n\n# create pairplot by quality\npp = sns.pairplot(gr8_wq, hue=\"quality\",diag_kind=\"hist\")\npp.fig.suptitle(\"Pairplot\", fontsize=30, y=1.02);\n\n\n\n\n\n\n\n\n\n\n3.4 Show correlations of wine features and wine quality scores\n\n# plot correltion heatmap\nplt.figure(figsize=(18,13))\nplt.title(\"Correlation - wine features vs. wine quality scores\", fontsize=25, pad=20)\nsns.set(font_scale=1.3)\ncorr = sns.heatmap(gr8_wq.corr(), annot=True, fmt='.2f', linewidths=2, cmap=\"crest\")\ncorr.set_yticklabels(corr.get_ymajorticklabels(), fontsize = 16);\ncorr.set_xticklabels(corr.get_xmajorticklabels(), fontsize = 16, rotation=45);\n\n\n\n\n\n\n\n\n\nThe correlation heatmap above shows that “Alcohol”, “Sulphates”, “Citric acid” these three features have high positive correlations with “Wine quality score”; “Volatile acidity”, “Total sulfur dioxide” have the high negative correlation with “Wine quality score”.\n\n\n\n3.5 Graph distributions of wine features (“Alcohol”, “Sulphates”, “Citric acid”,“Volatile acidity”, “Total sulfur dioxide”,“Chlorides”) by wine quality scores\n\n# Wine features distribution plots by wine quality score using Kernel density estimation (KDE)\nfig, axes = plt.subplots(2, 3, figsize=(20,13))\nfig.suptitle('Wine features density distributions by wine quality scores',fontsize=25, y=1.02)\n\nsns.kdeplot(ax=axes[0, 0], data=gr8_wq, x=\"alcohol\", hue=\"quality\", fill=True, palette='gist_earth');\nsns.kdeplot(ax=axes[0, 1], data=gr8_wq, x=\"sulphates\", hue=\"quality\", fill=True, palette='gist_earth');\nsns.kdeplot(ax=axes[0, 2], data=gr8_wq, x=\"citric acid\", hue=\"quality\", fill=True, palette='gist_earth');\nsns.kdeplot(ax=axes[1, 0], data=gr8_wq, x=\"volatile acidity\", hue=\"quality\", fill=True, palette='gist_earth');\nsns.kdeplot(ax=axes[1, 1], data=gr8_wq, x=\"total sulfur dioxide\", hue=\"quality\", fill=True, palette='gist_earth');\nsns.kdeplot(ax=axes[1, 2], data=gr8_wq, x=\"chlorides\", hue=\"quality\", fill=True, palette='gist_earth');\n\n\n\n\n\n\n\n\n\n\nThe distribution plots of above features reflect the same result as correlation heatmap: high quality wines have higher probability to have high level of Alcohol, Sulphates, Citric acid and Volatile acidity; low level of Total Sulfure dioxide. While low quality wines are in the opposite.\nnote that 2 = high quality, 1 = medium quality, 0 = low quality\n\n # Convert \"Quality\" into Low/Medium/High\ngr8_wq['quality_cat'] = np.select([gr8_wq.quality &gt;=7, gr8_wq.quality&gt;=5 , gr8_wq.quality&lt; 5], \n                    [2,1,0], \n                    default=None)\ngr8_wq['quality_cat'] = gr8_wq['quality_cat'].astype('category',copy=False)\n\n\ngr8_wq.head(10)\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\nquality_cat\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n1\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n1\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n1\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n1\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n1\n\n\n5\n7.4\n0.66\n0.00\n1.8\n0.075\n13.0\n40.0\n0.9978\n3.51\n0.56\n9.4\n5\n1\n\n\n6\n7.9\n0.60\n0.06\n1.6\n0.069\n15.0\n59.0\n0.9964\n3.30\n0.46\n9.4\n5\n1\n\n\n7\n7.3\n0.65\n0.00\n1.2\n0.065\n15.0\n21.0\n0.9946\n3.39\n0.47\n10.0\n7\n2\n\n\n8\n7.8\n0.58\n0.02\n2.0\n0.073\n9.0\n18.0\n0.9968\n3.36\n0.57\n9.5\n7\n2\n\n\n9\n7.5\n0.50\n0.36\n6.1\n0.071\n17.0\n102.0\n0.9978\n3.35\n0.80\n10.5\n5\n1\n\n\n\n\n\n\n\n\n# # pairplot by quality category\n# pp2 = sns.pairplot(gr8_wq, hue=\"quality_cat\", diag_kind=\"hist\")\n# pp2.fig.suptitle(\"Pairplot\", fontsize=30, y=1.02);\n\n\n# # Wine features distribution plots by wine quality cateogries using Kernel density estimation (KDE)\n# fig2, axes2 = plt.subplots(2, 3, figsize=(20,13))\n# fig2.suptitle('Wine features density distributions by wine quality category',fontsize=25, y=1.02)\n\n# sns.kdeplot(ax=axes2[0, 0], data=gr8_wq, x=\"alcohol\", hue=\"quality_cat\", fill=True, palette='gist_earth');\n# sns.kdeplot(ax=axes2[0, 1], data=gr8_wq, x=\"sulphates\", hue=\"quality_cat\", fill=True, palette='gist_earth');\n# sns.kdeplot(ax=axes2[0, 2], data=gr8_wq, x=\"citric acid\", hue=\"quality_cat\", fill=True, palette='gist_earth');\n# sns.kdeplot(ax=axes2[1, 0], data=gr8_wq, x=\"volatile acidity\", hue=\"quality_cat\", fill=True, palette='gist_earth');\n# sns.kdeplot(ax=axes2[1, 1], data=gr8_wq, x=\"total sulfur dioxide\", hue=\"quality_cat\", fill=True, palette='gist_earth');\n# sns.kdeplot(ax=axes2[1, 2], data=gr8_wq, x=\"chlorides\", hue=\"quality_cat\", fill=True, palette='gist_earth');\n\n\n\n\n\nSecond Section: Modeling\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nSplit the Dataset\nsplit out target variable\n\n\ngr8_X = gr8_wq.drop(['quality'], axis = 1)\n\ngr8_y = gr8_wq['quality']\n\n\n\ngr8_X\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\n\n\n\n\n0\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n\n\n1\n7.8\n0.880\n0.00\n2.6\n0.098\n25.0\n67.0\n0.99680\n3.20\n0.68\n9.8\n\n\n2\n7.8\n0.760\n0.04\n2.3\n0.092\n15.0\n54.0\n0.99700\n3.26\n0.65\n9.8\n\n\n3\n11.2\n0.280\n0.56\n1.9\n0.075\n17.0\n60.0\n0.99800\n3.16\n0.58\n9.8\n\n\n4\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1594\n6.2\n0.600\n0.08\n2.0\n0.090\n32.0\n44.0\n0.99490\n3.45\n0.58\n10.5\n\n\n1595\n5.9\n0.550\n0.10\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n\n\n1596\n6.3\n0.510\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n\n\n1597\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n\n\n1598\n6.0\n0.310\n0.47\n3.6\n0.067\n18.0\n42.0\n0.99549\n3.39\n0.66\n11.0\n\n\n\n\n1599 rows × 11 columns\n\n\n\n\ngr8_y\n\n0       5\n1       5\n2       5\n3       6\n4       5\n       ..\n1594    5\n1595    6\n1596    6\n1597    5\n1598    6\nName: quality, Length: 1599, dtype: int64\n\n\nsplit out train test dataset\n\ngr8_X_train, gr8_X_test, gr8_y_train, gr8_y_test = train_test_split(gr8_X, gr8_y, stratify=gr8_y,test_size=0.20)\n\nNote: Due to the small amount of High quality and low quality (8 & 3), the train test split is performed with stratify split instead of random split to ensure there are samiliar percentage of those minor labels in the test dataset\n\n\nBaseline model\nGet the baseline model accuracy\nNote: The baseline model we chose is logistic regression, no tuning performed\n\n# Import Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\n\n# Initializing the LogisticRegression Model\nmodel = LogisticRegression()\n\n# Training the model\nmodel.fit(gr8_X_train, gr8_y_train)\n\n# Making predictions on the datatest\ngr8_y_pred = model.predict(gr8_X_test)\n\n# Getting the score\n\nLogisticRegression_score = model.score(gr8_X_test, gr8_y_test)\nprint(\"The accuracy is:\", LogisticRegression_score)\n\nThe accuracy is: 0.553125\n\n\nModel coefficient\n\ncoefficient = model.coef_\n\n\ncoefficient_df = pd.DataFrame(coefficient, index = model.classes_, columns = gr8_X_train.columns)\ncoefficient_df.index.name = \"quality\"\ncoefficient_df\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\n\n\nquality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n0.067942\n0.073006\n-0.012764\n0.061425\n0.010225\n0.075731\n-0.034026\n-0.006234\n-0.010061\n-0.020839\n-0.249801\n\n\n4\n-0.228889\n0.237313\n-0.126594\n0.277913\n-0.011500\n-0.034861\n0.003799\n0.022120\n0.170797\n-0.082843\n0.026391\n\n\n5\n0.206450\n0.943529\n-0.453357\n-0.099189\n0.108902\n-0.006237\n0.026014\n0.467504\n1.678355\n-0.353553\n-0.693206\n\n\n6\n0.118011\n-0.400165\n0.083803\n-0.103738\n-0.018482\n0.020496\n0.005914\n-0.094054\n-0.369139\n0.196136\n0.216920\n\n\n7\n0.127973\n-0.768955\n0.460696\n-0.065319\n-0.075128\n0.026533\n-0.006653\n-0.339183\n-1.280072\n0.221368\n0.454996\n\n\n8\n-0.291487\n-0.084729\n0.048216\n-0.071092\n-0.014017\n-0.081662\n0.004952\n-0.050154\n-0.189879\n0.039732\n0.244700\n\n\n\n\n\n\n\nCoefficient visualization\n\nplt.figure(figsize=(10, 10),dpi=300)\ncorrelation_matrix = gr8_wq.corr().round(2)\nsns.heatmap(data=correlation_matrix, annot=True)\n#plt.savefig('/Users/Ahmed/Documents/Dsan6700/heatmap.jpg',dpi=300)\n\n\n\n\n\n\n\n\n\n\nEvaluate base models\nTest performance of base models\nNote: Due to the new version of the XGBoost model’s issue, y_train must be encoded.\n\n# #encode y_train\n# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# y_train = le.fit_transform(y_train)\n\n\n# list of models\ndef base_models():\n    models = dict()\n    models['lr'] = LogisticRegression()\n    models[\"KN\"] = KNeighborsClassifier()\n    models[\"SVC\"] = svm.SVC()\n    models[\"Tree\"] = DecisionTreeClassifier()\n    models[\"Random Forest\"] = RandomForestClassifier()\n    models[\"Bagging\"] = BaggingClassifier()\n    models[\"GBM\"] = GradientBoostingClassifier()\n    models[\"GNB\"] = GaussianNB()\n    # models[\"XGB\"] = XGBClassifier()\n    return models\n\n\n# Function to evaluate the list of models\ndef eval_models(model):\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = -cross_val_score(model, gr8_X_train, gr8_y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n\n# evaluate the models and print results\nmodels = base_models()\nresults, names = list(), list() \nfor name, model in models.items():\n  scores = eval_models(model)\n  results.append(scores)\n  names.append(name)\n  print('&gt;%s %.3f (%.3f)' % (name, scores.mean(), scores.std()))\n\n# store results in dataframe\nclassmod = pd.DataFrame(np.transpose(results), columns = [\"lr\",\"KN\",\"SVC\",\"Tree\",\"Random Forest\",\"Bagging\",\"GBM\", \"GNB\"])\nclassmod = pd.melt(classmod.reset_index(), id_vars='index',value_vars=[\"lr\",\"KN\",\"SVC\",\"Tree\",\"Random Forest\",\"Bagging\",\"GBM\", \"GNB\"])\n\n&gt;lr 0.470 (0.052)\n&gt;KN 0.598 (0.057)\n&gt;SVC 0.559 (0.046)\n&gt;Tree 0.490 (0.059)\n&gt;Random Forest 0.363 (0.049)\n&gt;Bagging 0.407 (0.060)\n&gt;GBM 0.422 (0.047)\n&gt;GNB 0.565 (0.069)\n\n\nVisualize base models’ performance\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nfig = px.box(classmod, x=\"variable\", y=\"value\",color=\"variable\",points='all',\nlabels={\"variable\": \"Machine Learning Model\",\n        \"value\": \"RMS Error\"\n        },title=\"Model Performance\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nBuild and apply Stack model base on the performance of base models\n\nfrom sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier,RandomForestClassifier, StackingClassifier\n\nSelect base models to stacking model\nNote that the base models with best performance are Random Forest, Bagging and GBM\n\ndef get_stacking():\n# define the base models\n    level0 = list()\n    level0.append(('Bagging', BaggingClassifier()))\n    level0.append(('RF', RandomForestClassifier()))\n    level0.append(('GBM', GradientBoostingClassifier()))\n# define meta learner model\n    level1 = LogisticRegression()\n# define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model\ndef base_models():\n    models = dict()\n    models[\"Bagging\"] = BaggingClassifier()\n    models[\"Random Forest\"] = RandomForestClassifier()\n    models[\"GBM\"] = GradientBoostingClassifier()\n    models[\"Stacked Model\"] = get_stacking()\n    return models\n\nNew models evaluation\n\ndef eval_models(model):\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = -cross_val_score(model, gr8_X_train, gr8_y_train,\n    scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1,\n    error_score='raise')\n    return scores\nmodels = base_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = eval_models(model)\n    results.append(scores)\n    names.append(name)\n    print('&gt;%s %.3f (%.3f)' % (name, scores.mean(), scores.std()))\nclassmod = pd.DataFrame(np.transpose(results), columns = [\"Bagging\",\"Random Forest\",\"GBM\",\"Stacked Model\"])\nclassmod = pd.melt(classmod.reset_index(),\nid_vars='index',value_vars=[\"Bagging\",\"Random Forest\",\"GBM\",\"Stacked Model\"])\nfig = px.box(classmod, x=\"variable\", y=\"value\",color=\"variable\",points='all',\nlabels={\"variable\": \"Machine Learning Model\",\"value\": \"RMS Error\"},title=\"Model Performance\")\nfig.show()\n# fig.write_image(\"Project/Boxplot-candidate.jpeg\",engine=\"kaleido\",format=\"png\",width=1600, height=700, scale=0.75)fig.show()\n\n&gt;Bagging 0.407 (0.053)\n&gt;Random Forest 0.356 (0.045)\n&gt;GBM 0.417 (0.048)\n&gt;Stacked Model 0.356 (0.042)\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nExport Pickle model and use the model to predict y_test from x_test\n\nimport pickle\n#library loaded\ngr8_y_train = gr8_y_train.ravel()\nlevel0 = list()\nlevel0.append(('Bagging', BaggingClassifier()))\nlevel0.append(('RF', RandomForestClassifier()))\nlevel0.append(('GBM', GradientBoostingClassifier()))\nlevel1 = LogisticRegression()\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\nmodel.fit(gr8_X_train, gr8_y_train)\n#Save to file in the current working directory\npkl_filename = \"./app/TrainedModel/AssignmentPickle.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(model, file)\n# Load the model from file\nwith open(pkl_filename, 'rb') as file:\n    pickle_model = pickle.load(file)\n    \nscore = pickle_model.score(gr8_X_test, gr8_y_test)\nprint(\"Test score: {0:.2f} %\".format(100 * score))\ngr8_Y_predict = pickle_model.predict(gr8_X_test)\n\nTest score: 71.88 %\n\n\nGet the confusion matrix\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\npredictions = pd.DataFrame(gr8_Y_predict, columns=['predictions'])\npredictions['actual'] = gr8_y_test\nprint(f'The confusion matrix for the model is: \\n {confusion_matrix( gr8_y_test, gr8_Y_predict.ravel())}')\n\n#Print the confusion matrix below\n\nThe confusion matrix for the model is: \n [[  0   0   2   0   0   0]\n [  0   0   9   2   0   0]\n [  0   0 113  23   0   0]\n [  0   0  27  91  10   0]\n [  0   0   0  14  26   0]\n [  0   0   0   3   0   0]]\n\n\nVisualize the confusion matrix\n\npd.Series(data =list(gr8_y_train)).value_counts()\n\n5    545\n6    510\n7    159\n4     42\n8     15\n3      8\nName: count, dtype: int64\n\n\n\ngr8_y_test.value_counts()\n\nquality\n5    136\n6    128\n7     40\n4     11\n8      3\n3      2\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom seaborn import set_palette\n\nplt.figure(figsize=(2.5,2.5),dpi=75)\nset_palette(\"Paired\")\n\nG8_conf_matrix = confusion_matrix(gr8_y_test, gr8_Y_predict)\n#ConfusionMatrixDisplay.from_estimator(pickle_model, gr8_X_test, gr8_y_test)\n\n&lt;Figure size 187.5x187.5 with 0 Axes&gt;\n\n\nAdd predicted value to the original dataset\n\n# graph confusion matrix\n# plot correltion heatmap\nplt.figure(figsize = (15,10))\n\nplt.title(\"Predicted Quality vs. Quality Confusion Matrix\", fontsize=25, pad=20)\n\nsns.set(font_scale=1.3)\ncm = sns.heatmap(G8_conf_matrix, annot=True,fmt='.0f', cmap='crest')\ncm.set_yticklabels(cm.get_ymajorticklabels(), fontsize = 16);\ncm.set_xticklabels(cm.get_xmajorticklabels(), fontsize = 16);\n\n\n\n\n\n\n\n\n\n\ngr8_wq['Predict_quality'] = pickle_model.predict(gr8_X)\n\n\ngr8_wq.to_csv('gr8_wq_qauality.csv',index=False)"
  },
  {
    "objectID": "Logistic_regression.html",
    "href": "Logistic_regression.html",
    "title": "Wine Quality Prediction",
    "section": "",
    "text": "# Import the necessary libaries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Loading the dataset\ngr8_project = pd.read_csv(\"datasets/winequality-red.csv\", header = 0)\ngr8_project\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.880\n0.00\n2.6\n0.098\n25.0\n67.0\n0.99680\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.760\n0.04\n2.3\n0.092\n15.0\n54.0\n0.99700\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.280\n0.56\n1.9\n0.075\n17.0\n60.0\n0.99800\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1594\n6.2\n0.600\n0.08\n2.0\n0.090\n32.0\n44.0\n0.99490\n3.45\n0.58\n10.5\n5\n\n\n1595\n5.9\n0.550\n0.10\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n6\n\n\n1596\n6.3\n0.510\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n6\n\n\n1597\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n5\n\n\n1598\n6.0\n0.310\n0.47\n3.6\n0.067\n18.0\n42.0\n0.99549\n3.39\n0.66\n11.0\n6\n\n\n\n\n1599 rows × 12 columns\n\n\n\n\ngr8_project.head()\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n\n\n\n\n\n\ngr8_project['quality'].describe()\n\ncount    1599.000000\nmean        5.636023\nstd         0.807569\nmin         3.000000\n25%         5.000000\n50%         6.000000\n75%         6.000000\nmax         8.000000\nName: quality, dtype: float64\n\n\n\n# Train test split\n\nfrom sklearn.model_selection import train_test_split\nX = gr8_project.drop('quality', axis = 1)\n\ny = gr8_project['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n\n# Import Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\n\n# Initializing the LogisticRegression Model\nmodel = LogisticRegression()\n\n# Training the model\nmodel.fit(X_train, y_train)\n\n# Making predictions on the datatest\ny_pred = model.predict(X_test)\n\n# Getting the score\n\nLogisticRegression_score = model.score(X_test, y_test)\nprint(\"The accuracy is:\", LogisticRegression_score)\n\n\nThe accuracy is: 0.60625\n\n\n\ny_train.unique()\n\narray([7, 8, 5, 6, 4, 3])\n\n\n\ncoefficient = model.coef_\n\n\ncoefficient_df = pd.DataFrame(coefficient, index = model.classes_, columns = X_train.columns)\ncoefficient_df.index.name = \"quality\"\ncoefficient_df\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\n\n\nquality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n0.108662\n0.047814\n-0.001399\n-0.117974\n0.010206\n0.068470\n-0.025349\n-0.007586\n-0.021292\n-0.014301\n-0.274193\n\n\n4\n-0.216638\n0.257724\n-0.090816\n0.226146\n0.009100\n-0.074989\n0.014977\n0.037795\n0.191897\n-0.065055\n0.041020\n\n\n5\n0.228524\n0.961999\n-0.461900\n-0.066535\n0.117937\n-0.025906\n0.031717\n0.474425\n1.672060\n-0.342484\n-0.713056\n\n\n6\n0.089390\n-0.427785\n0.027305\n-0.063529\n-0.045297\n0.004009\n0.011798\n-0.097105\n-0.327644\n0.169918\n0.226219\n\n\n7\n0.085033\n-0.752602\n0.471927\n0.004572\n-0.079832\n0.026176\n-0.005375\n-0.351099\n-1.301671\n0.232901\n0.472172\n\n\n8\n-0.294971\n-0.087151\n0.054884\n0.017320\n-0.012114\n0.002241\n-0.027767\n-0.056431\n-0.213351\n0.019020\n0.247837\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 10),dpi=300)\ncorrelation_matrix = gr8_project.corr().round(2)\nsns.heatmap(data=correlation_matrix, annot=True)\n#plt.savefig('/Users/Ahmed/Documents/Dsan6700/heatmap.jpg',dpi=300)"
  },
  {
    "objectID": "wine_quality_prediction.html#eda",
    "href": "wine_quality_prediction.html#eda",
    "title": "Wine Quality Prediction Using Stacking Model And Hyperparameter Tuning",
    "section": "EDA",
    "text": "EDA\n\nPairplot\nCreate pairplot to observe the relationship between variables colored by “quality”\n\n# create pairplot\n# pp = sns.pairplot(wq_data, hue=\"quality\",diag_kind=\"hist\")\n# pp.fig.suptitle(\"Pairplot\", fontsize=30, y=1.02);"
  }
]